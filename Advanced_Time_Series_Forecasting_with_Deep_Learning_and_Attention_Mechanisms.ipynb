{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "yF8HLNZWXEGj",
        "outputId": "675eb8e9-b1e4-48db-873e-62b7519be3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.6)\n",
            "Requirement already satisfied: prophet in /usr/local/lib/python3.12/dist-packages (1.3.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.2)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.12/dist-packages (from prophet) (1.3.0)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.12/dist-packages (from prophet) (0.90)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.12/dist-packages (from prophet) (4.67.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from prophet) (6.5.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.18.3)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.46)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading optuna-4.7.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.7.0\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas matplotlib torch scikit-learn statsmodels prophet optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from prophet import Prophet\n",
        "import optuna\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==========================================\n",
        "# 1. DATA GENERATION\n",
        "# ==========================================\n",
        "def generate_multivariate_data(n_points=2000):\n",
        "    np.random.seed(42)\n",
        "    t = np.arange(n_points)\n",
        "\n",
        "    # Simulate seasonality, trend, and noise\n",
        "    series1 = 10 + 0.05 * t + 10 * np.sin(2 * np.pi * t / 50) + np.random.normal(0, 1, n_points)\n",
        "    series2 = 20 + 0.02 * t + 5 * np.cos(2 * np.pi * t / 100) + np.random.normal(0, 0.5, n_points)\n",
        "\n",
        "    # Target variable influenced by both series with a lag\n",
        "    target = 0.6 * series1 + 0.4 * series2 + np.random.normal(0, 2, n_points)\n",
        "\n",
        "    df = pd.DataFrame({'feature1': series1, 'feature2': series2, 'target': target})\n",
        "    return df\n",
        "\n",
        "# Data Preparation for Deep Learning\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:(i + seq_length), :]\n",
        "        y = data[i + seq_length, -1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# ==========================================\n",
        "# 2. DEEP LEARNING MODEL\n",
        "# ==========================================\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        attn_weights = torch.tanh(self.attn(lstm_output))\n",
        "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
        "        context = torch.sum(attn_weights * lstm_output, dim=1)\n",
        "        return context, attn_weights\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim=1):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        context, weights = self.attention(lstm_out)\n",
        "        out = self.fc(context)\n",
        "        return out\n",
        "\n",
        "# ==========================================\n",
        "# 3. BASELINE MODELS\n",
        "# ==========================================\n",
        "def train_baselines(train_df, test_df):\n",
        "    print(\"Training Baselines...\")\n",
        "    # SARIMA\n",
        "    sarima_model = SARIMAX(train_df['target'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
        "    sarima_res = sarima_model.fit(disp=False)\n",
        "    sarima_pred = sarima_res.forecast(steps=len(test_df))\n",
        "\n",
        "    # Prophet\n",
        "    prophet_df = train_df.reset_index().rename(columns={'index': 'ds', 'target': 'y'})\n",
        "    prophet_df['ds'] = pd.date_range(start='2020-01-01', periods=len(prophet_df), freq='D')\n",
        "    m = Prophet(yearly_seasonality=True, daily_seasonality=False)\n",
        "    m.fit(prophet_df)\n",
        "    future = m.make_future_dataframe(periods=len(test_df))\n",
        "    forecast = m.predict(future)\n",
        "    prophet_pred = forecast['yhat'].iloc[-len(test_df):].values\n",
        "\n",
        "    return sarima_pred, prophet_pred\n",
        "\n",
        "# ==========================================\n",
        "# 4. HYPERPARAMETER TUNING & EVALUATION\n",
        "# ==========================================\n",
        "def objective(trial, X_train, y_train):\n",
        "    hidden_dim = trial.suggest_int('hidden_dim', 16, 64)\n",
        "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
        "\n",
        "    model = AttentionLSTM(input_dim=3, hidden_dim=hidden_dim, num_layers=1)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop for tuning\n",
        "    model.train()\n",
        "    for epoch in range(10):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_train)\n",
        "        loss = criterion(output.squeeze(), y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# ==========================================\n",
        "# MAIN EXECUTION FLOW\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Data Prep\n",
        "    df = generate_multivariate_data()\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "    SEQ_LENGTH = 20\n",
        "    X, y = create_sequences(scaled_data, SEQ_LENGTH)\n",
        "\n",
        "    split = int(0.8 * len(X))\n",
        "    X_train, X_test = torch.FloatTensor(X[:split]), torch.FloatTensor(X[split:])\n",
        "    y_train, y_test = torch.FloatTensor(y[:split]), torch.FloatTensor(y[split:])\n",
        "\n",
        "    # Optuna Tuning\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=5)\n",
        "\n",
        "    # Final Model Training\n",
        "    best_params = study.best_params\n",
        "    model = AttentionLSTM(3, best_params['hidden_dim'], 1)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
        "\n",
        "    print(\"Training Final Attention-LSTM model...\")\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X_train)\n",
        "        loss = nn.MSELoss()(out.squeeze(), y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 10 == 0: print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(X_test).numpy()\n",
        "\n",
        "    # Baselines\n",
        "    sarima_p, prophet_p = train_baselines(df.iloc[:split], df.iloc[split:])\n",
        "\n",
        "    # Metrics\n",
        "    print(\"\\n--- PERFORMANCE SUMMARY ---\")\n",
        "    for name, p in zip(['Attention-LSTM', 'SARIMA', 'Prophet'], [preds.flatten(), sarima_p, prophet_p]):\n",
        "\n",
        "        mae = mean_absolute_error(df['target'].iloc[-len(p):], p)\n",
        "        print(f\"{name} -> MAE: {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BHPbbeeMXd9h",
        "outputId": "66b2d843-bae9-49b7-99be-6d2ee480c968"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-02-06 09:54:37,558] A new study created in memory with name: no-name-0904d355-ccd9-40dc-8e46-9fbe4fd4818c\n",
            "[I 2026-02-06 09:54:46,534] Trial 0 finished with value: 0.17119081318378448 and parameters: {'hidden_dim': 50, 'lr': 0.0004494520184143668}. Best is trial 0 with value: 0.17119081318378448.\n",
            "[I 2026-02-06 09:54:48,913] Trial 1 finished with value: 0.17057006061077118 and parameters: {'hidden_dim': 58, 'lr': 0.0009656702589346824}. Best is trial 1 with value: 0.17057006061077118.\n",
            "[I 2026-02-06 09:54:50,752] Trial 2 finished with value: 0.3201044797897339 and parameters: {'hidden_dim': 38, 'lr': 0.0004756419905802464}. Best is trial 1 with value: 0.17057006061077118.\n",
            "[I 2026-02-06 09:54:51,832] Trial 3 finished with value: 0.1034071072936058 and parameters: {'hidden_dim': 32, 'lr': 0.000892925158312943}. Best is trial 3 with value: 0.1034071072936058.\n",
            "[I 2026-02-06 09:54:55,072] Trial 4 finished with value: 0.03533673658967018 and parameters: {'hidden_dim': 62, 'lr': 0.003349553406771809}. Best is trial 4 with value: 0.03533673658967018.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Final Attention-LSTM model...\n",
            "Epoch 0, Loss: 0.3122\n",
            "Epoch 10, Loss: 0.0110\n",
            "Epoch 20, Loss: 0.0106\n",
            "Epoch 30, Loss: 0.0074\n",
            "Epoch 40, Loss: 0.0052\n",
            "Training Baselines...\n",
            "\n",
            "--- PERFORMANCE SUMMARY ---\n",
            "Attention-LSTM -> MAE: 81.5133\n",
            "SARIMA -> MAE: 4.5770\n",
            "Prophet -> MAE: 4.6337\n"
          ]
        }
      ]
    }
  ]
}